{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c279c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- CONFIGURAÇÕES DE CAMINHO ---\n",
    "ARQUIVO_MODELO = \"vetorizador.pkl\"\n",
    "ARQUIVO_CLUSTERS = \"base_clusters.csv\"\n",
    "ARQUIVO_CENTROIDES = \"centroides.pkl\"\n",
    "\n",
    "# --- 1. FUNÇÃO DE LIMPEZA (O seu tratamento de texto) ---\n",
    "def limpar_texto(texto):\n",
    "    \"\"\"Remove caracteres especiais, acentos e padroniza para minúsculo.\"\"\"\n",
    "    if not isinstance(texto, str): return \"\"\n",
    "    texto = texto.lower()\n",
    "    # Mantém apenas letras de 'a' a 'z' e espaços\n",
    "    texto = re.sub(r'[^a-z\\s]', '', texto)\n",
    "    return texto.strip()\n",
    "\n",
    "# --- 2. FUNÇÕES DE CARGA E SALVAMENTO ---\n",
    "def carregar_estado():\n",
    "    \"\"\"Carrega os dados salvos anteriormente para continuar de onde parou.\"\"\"\n",
    "    vetorizador = None\n",
    "    clusters_df = pd.DataFrame()\n",
    "    centroides_lista = []\n",
    "\n",
    "    if os.path.exists(ARQUIVO_MODELO):\n",
    "        with open(ARQUIVO_MODELO, 'rb') as f:\n",
    "            vetorizador = pickle.load(f)\n",
    "    \n",
    "    if os.path.exists(ARQUIVO_CLUSTERS):\n",
    "        clusters_df = pd.read_csv(ARQUIVO_CLUSTERS)\n",
    "        # Converte o texto da coluna 'cpfs_unicos' de volta para um conjunto (set)\n",
    "        clusters_df['cpfs_unicos'] = clusters_df['cpfs_unicos'].apply(eval).apply(set)\n",
    "    \n",
    "    if os.path.exists(ARQUIVO_CENTROIDES):\n",
    "        with open(ARQUIVO_CENTROIDES, 'rb') as f:\n",
    "            centroides_lista = pickle.load(f)\n",
    "            \n",
    "    return vetorizador, clusters_df, centroides_lista\n",
    "\n",
    "def salvar_estado(vetorizador, clusters_df, centroides_lista):\n",
    "    \"\"\"Salva os dados atuais em arquivos para persistência.\"\"\"\n",
    "    with open(ARQUIVO_MODELO, 'wb') as f:\n",
    "        pickle.dump(vetorizador, f)\n",
    "    \n",
    "    with open(ARQUIVO_CENTROIDES, 'wb') as f:\n",
    "        pickle.dump(centroides_lista, f)\n",
    "        \n",
    "    # No CSV, salvamos o set de CPFs como lista para ser legível\n",
    "    df_temp = clusters_df.copy()\n",
    "    df_temp['cpfs_unicos'] = df_temp['cpfs_unicos'].apply(list)\n",
    "    df_temp.to_csv(ARQUIVO_CLUSTERS, index=False)\n",
    "\n",
    "# --- 3. LÓGICA PRINCIPAL (INCREMENTAL) ---\n",
    "def processar_dados(df_novo, limiar=0.60, min_cpfs=3):\n",
    "    # Tenta carregar o que já existe\n",
    "    vetorizador, clusters_df, centroides_lista = carregar_estado()\n",
    "\n",
    "    # Se o vetorizador não existir, cria um novo (Treino inicial)\n",
    "    if vetorizador is None:\n",
    "        print(\"Treinando vetorizador pela primeira vez...\")\n",
    "        try:\n",
    "            stops = stopwords.words('portuguese')\n",
    "        except:\n",
    "            nltk.download('stopwords')\n",
    "            stops = stopwords.words('portuguese')\n",
    "        \n",
    "        vetorizador = TfidfVectorizer(stop_words=stops, max_features=5000)\n",
    "        textos_treino = [limpar_texto(t) for t in df_novo['texto']]\n",
    "        vetorizador.fit(textos_treino)\n",
    "\n",
    "    # Prepara os textos novos\n",
    "    textos_limpos = [limpar_texto(t) for t in df_novo['texto']]\n",
    "    matriz_novos_vetores = vetorizador.transform(textos_limpos)\n",
    "    \n",
    "    resultados = []\n",
    "\n",
    "    # Percorre cada texto novo\n",
    "    for i in range(matriz_novos_vetores.shape[0]):\n",
    "        vetor_atual = matriz_novos_vetores[i]\n",
    "        cpf_atual = str(df_novo.iloc[i]['cpf'])\n",
    "        id_atual = df_novo.iloc[i]['id']\n",
    "        \n",
    "        melhor_cluster_idx = -1\n",
    "        maior_score = 0.0\n",
    "\n",
    "        # Compara com os centroides existentes\n",
    "        if len(centroides_lista) > 0:\n",
    "            matriz_centroides = np.vstack(centroides_lista)\n",
    "            sims = cosine_similarity(vetor_atual, matriz_centroides).flatten()\n",
    "            melhor_cluster_idx = np.argmax(sims)\n",
    "            maior_score = sims[melhor_cluster_idx]\n",
    "\n",
    "        # SE encontrar similaridade >= 60%\n",
    "        if melhor_cluster_idx != -1 and maior_score >= limiar:\n",
    "            # Pega o ID do cluster encontrado\n",
    "            c_id = clusters_df.iloc[melhor_cluster_idx]['cluster_id']\n",
    "            \n",
    "            # Atualiza metadados (CPFs e Quantidade)\n",
    "            clusters_df.at[melhor_cluster_idx, 'cpfs_unicos'].add(cpf_atual)\n",
    "            clusters_df.at[melhor_cluster_idx, 'quantidade_mensagens'] += 1\n",
    "            \n",
    "            # Verifica se virou influenciador\n",
    "            if len(clusters_df.at[melhor_cluster_idx, 'cpfs_unicos']) >= min_cpfs:\n",
    "                clusters_df.at[melhor_cluster_idx, 'e_influenciador'] = True\n",
    "            \n",
    "            # Recalcula o centroide (Média móvel para o grupo evoluir)\n",
    "            n = clusters_df.at[melhor_cluster_idx, 'quantidade_mensagens']\n",
    "            antigo_centroide = centroides_lista[melhor_cluster_idx]\n",
    "            novo_centroide = (antigo_centroide * (n-1) + vetor_atual.toarray()) / n\n",
    "            centroides_lista[melhor_cluster_idx] = novo_centroide\n",
    "            \n",
    "            cluster_final = c_id\n",
    "        else:\n",
    "            # SE NÃO encontrar, cria um cluster NOVO\n",
    "            cluster_final = len(clusters_df) + 1\n",
    "            novo_reg = {\n",
    "                'cluster_id': cluster_final,\n",
    "                'quantidade_mensagens': 1,\n",
    "                'cpfs_unicos': {cpf_atual},\n",
    "                'e_influenciador': False\n",
    "            }\n",
    "            clusters_df = pd.concat([clusters_df, pd.DataFrame([novo_reg])], ignore_index=True)\n",
    "            centroides_lista.append(vetor_atual.toarray())\n",
    "            maior_score = 1.0 # Similaridade com ele mesmo ao criar\n",
    "\n",
    "        # Adiciona ao relatório de saída\n",
    "        e_influ = clusters_df[clusters_df['cluster_id'] == cluster_final]['e_influenciador'].values[0]\n",
    "        resultados.append({\n",
    "            'id': id_atual,\n",
    "            'cluster_atribuido': cluster_final,\n",
    "            'similaridade': round(float(maior_score), 4),\n",
    "            'influenciador': 1 if e_influ else 0\n",
    "        })\n",
    "\n",
    "    # Salva tudo nos arquivos para a próxima rodada\n",
    "    salvar_estado(vetorizador, clusters_df, centroides_lista)\n",
    "    \n",
    "    # Retorna o DataFrame com as novas colunas\n",
    "    return pd.merge(df_novo, pd.DataFrame(resultados), on='id')\n",
    "\n",
    "# --- EXECUÇÃO ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Dados de exemplo (Simulando o df_influ)\n",
    "    meus_dados = pd.DataFrame({\n",
    "        'id': [1, 2, 3],\n",
    "        'texto': [\"Problema com o pix fora do ar\", \"Pix nao funciona\", \"Meu cartao atrasou\"],\n",
    "        'cpf': ['111', '222', '333']\n",
    "    })\n",
    "\n",
    "    # Chama a função principal\n",
    "    resultado_final = processar_dados(meus_dados)\n",
    "    print(resultado_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8422e948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULTADO DO PROCESSAMENTO ---\n",
      "    id                                        texto  cpf  cluster_atribuido  \\\n",
      "0    7       Erro generalizado no pix, arrumem logo  666                 13   \n",
      "1    8                      Pix travado aqui também  222                 14   \n",
      "2    9  Estou aguardando a entrega do cartão a dias  777                 15   \n",
      "3   10         Atendimento péssimo ninguem responde  888                 16   \n",
      "4   11               Vocês são horriveis me atendam  897                 17   \n",
      "5   12    Vou processar voces pelo atendimento ruim  888                 18   \n",
      "6   13                  O aplicativo é muito bonito  234                 19   \n",
      "7   14          Alguém me ajuda com o pix por favor  345                 20   \n",
      "8   15                    Alguém me ajuda com o pix  667                 21   \n",
      "9   16                 me ajuda com o pix por favor  245                 22   \n",
      "10  17             O pix está fora do ar desde cedo  756                 23   \n",
      "\n",
      "    score_similaridade  status_influenciador  \n",
      "0                  0.0                     0  \n",
      "1                  0.0                     0  \n",
      "2                  0.0                     0  \n",
      "3                  0.0                     0  \n",
      "4                  0.0                     0  \n",
      "5                  0.0                     0  \n",
      "6                  0.0                     0  \n",
      "7                  0.0                     0  \n",
      "8                  0.0                     0  \n",
      "9                  0.0                     0  \n",
      "10                 0.0                     0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- CONFIGURAÇÕES DE ARQUIVOS ---\n",
    "CAMINHO_MODELO = \"vetorizador_tfidf.pkl\"\n",
    "CAMINHO_CLUSTERS = \"base_clusters.csv\"\n",
    "CAMINHO_CENTROIDES = \"centroides_matriz.pkl\"\n",
    "\n",
    "class ProcessadorTexto:\n",
    "    \"\"\"Classe responsável pela limpeza padronizada dos textos\"\"\"\n",
    "    @staticmethod\n",
    "    def limpar(texto):\n",
    "        if not isinstance(texto, str): return \"\"\n",
    "        texto = texto.lower()\n",
    "        # Remove acentos e caracteres especiais\n",
    "        texto = re.sub(r'[^a-z\\s]', '', texto)\n",
    "        return texto.strip()\n",
    "\n",
    "class DetectorIncrementalInfluenciador:\n",
    "    def __init__(self, similaridade_minima=0.60, minimo_cpfs=3):\n",
    "        self.limiar = similaridade_minima\n",
    "        self.min_cpfs = minimo_cpfs\n",
    "        self.vetorizador = None\n",
    "        self.clusters_df = pd.DataFrame() # Memória dos metadados dos clusters\n",
    "        self.centroides_lista = []        # Memória dos vetores (centroides)\n",
    "        \n",
    "        # Tenta carregar dados existentes ao iniciar\n",
    "        self._carregar_estado()\n",
    "\n",
    "    def _carregar_estado(self):\n",
    "        \"\"\"Carrega os arquivos salvos se eles existirem\"\"\"\n",
    "        if os.path.exists(CAMINHO_MODELO):\n",
    "            with open(CAMINHO_MODELO, 'rb') as f:\n",
    "                self.vetorizador = pickle.load(f)\n",
    "        \n",
    "        if os.path.exists(CAMINHO_CLUSTERS):\n",
    "            self.clusters_df = pd.read_csv(CAMINHO_CLUSTERS)\n",
    "            # Converte a string de CPFs de volta para um conjunto (set)\n",
    "            self.clusters_df['cpfs_unicos'] = self.clusters_df['cpfs_unicos'].apply(eval).apply(set)\n",
    "        \n",
    "        if os.path.exists(CAMINHO_CENTROIDES):\n",
    "            with open(CAMINHO_CENTROIDES, 'rb') as f:\n",
    "                self.centroides_lista = pickle.load(f)\n",
    "\n",
    "    def _salvar_estado(self):\n",
    "        \"\"\"Persiste os dados em disco para a próxima execução\"\"\"\n",
    "        with open(CAMINHO_MODELO, 'wb') as f:\n",
    "            pickle.dump(self.vetorizador, f)\n",
    "        \n",
    "        with open(CAMINHO_CENTROIDES, 'wb') as f:\n",
    "            pickle.dump(self.centroides_lista, f)\n",
    "            \n",
    "        # Para salvar no CSV, convertemos o set de CPFs em lista (string)\n",
    "        df_para_salvar = self.clusters_df.copy()\n",
    "        df_para_salvar['cpfs_unicos'] = df_para_salvar['cpfs_unicos'].apply(list)\n",
    "        df_para_salvar.to_csv(CAMINHO_CLUSTERS, index=False)\n",
    "\n",
    "    def inicializar_modelo(self, textos_treino):\n",
    "        \"\"\"Treina o TF-IDF pela primeira vez (Cold Start)\"\"\"\n",
    "        from nltk.corpus import stopwords\n",
    "        import nltk\n",
    "        try:\n",
    "            pt_stops = stopwords.words('portuguese')\n",
    "        except:\n",
    "            nltk.download('stopwords')\n",
    "            pt_stops = stopwords.words('portuguese')\n",
    "\n",
    "        self.vetorizador = TfidfVectorizer(stop_words=pt_stops, max_features=5000)\n",
    "        textos_limpos = [ProcessadorTexto.limpar(t) for t in textos_treino]\n",
    "        self.vetorizador.fit(textos_limpos)\n",
    "        print(\"✅ Modelo TF-IDF inicializado e treinado.\")\n",
    "\n",
    "    def processar_lote_diario(self, novos_dados):\n",
    "        \"\"\"\n",
    "        Recebe um DataFrame com ['id', 'texto', 'cpf'].\n",
    "        Retorna o mesmo DataFrame com as colunas de cluster e influencer.\n",
    "        \"\"\"\n",
    "        textos_limpos = [ProcessadorTexto.limpar(t) for t in novos_dados['texto']]\n",
    "        matriz_vetores = self.vetorizador.transform(textos_limpos)\n",
    "        \n",
    "        resultados_finais = []\n",
    "\n",
    "        # Processa cada linha do novo lote\n",
    "        for i in range(matriz_vetores.shape[0]):\n",
    "            vetor_atual = matriz_vetores[i]\n",
    "            cpf_atual = str(novos_dados.iloc[i]['cpf'])\n",
    "            id_origem = novos_dados.iloc[i]['id']\n",
    "            \n",
    "            melhor_cluster_id = None\n",
    "            maior_similaridade = 0.0\n",
    "            \n",
    "            # 1. Comparação com clusters existentes\n",
    "            if len(self.centroides_lista) > 0:\n",
    "                # Transforma lista de centroides em matriz para cálculo rápido\n",
    "                matriz_centroides = np.vstack(self.centroides_lista)\n",
    "                similaridades = cosine_similarity(vetor_atual, matriz_centroides).flatten()\n",
    "                \n",
    "                indice_melhor = np.argmax(similaridades)\n",
    "                if similaridades[indice_melhor] >= self.limiar:\n",
    "                    maior_similaridade = similaridades[indice_melhor]\n",
    "                    melhor_cluster_id = self.clusters_df.iloc[indice_melhor]['cluster_id']\n",
    "\n",
    "            # 2. Lógica de Atribuição ou Criação\n",
    "            if melhor_cluster_id is not None:\n",
    "                # Atualiza cluster existente\n",
    "                idx = self.clusters_df[self.clusters_df['cluster_id'] == melhor_cluster_id].index[0]\n",
    "                \n",
    "                # Atualiza CPFs e contagem\n",
    "                self.clusters_df.at[idx, 'cpfs_unicos'].add(cpf_atual)\n",
    "                total_cpfs = len(self.clusters_df.at[idx, 'cpfs_unicos'])\n",
    "                self.clusters_df.at[idx, 'quantidade_mensagens'] += 1\n",
    "                \n",
    "                # Verifica regra de Influencer\n",
    "                if total_cpfs >= self.min_cpfs:\n",
    "                    self.clusters_df.at[idx, 'e_influenciador'] = True\n",
    "                \n",
    "                # Atualiza o centroide (média móvel)\n",
    "                n = self.clusters_df.at[idx, 'quantidade_mensagens']\n",
    "                novo_centroide = (self.centroides_lista[idx] * (n-1) + vetor_atual.toarray()) / n\n",
    "                self.centroides_lista[idx] = novo_centroide\n",
    "            else:\n",
    "                # Cria novo cluster\n",
    "                novo_id = len(self.clusters_df) + 1\n",
    "                novo_registro = {\n",
    "                    'cluster_id': novo_id,\n",
    "                    'quantidade_mensagens': 1,\n",
    "                    'cpfs_unicos': {cpf_atual},\n",
    "                    'e_influenciador': False\n",
    "                }\n",
    "                self.clusters_df = pd.concat([self.clusters_df, pd.DataFrame([novo_registro])], ignore_index=True)\n",
    "                self.centroides_lista.append(vetor_atual.toarray())\n",
    "                melhor_cluster_id = novo_id\n",
    "\n",
    "            # Guardar resultado da linha atual\n",
    "            status_influencer = self.clusters_df[self.clusters_df['cluster_id'] == melhor_cluster_id]['e_influenciador'].values[0]\n",
    "            resultados_finais.append({\n",
    "                'id': id_origem,\n",
    "                'cluster_atribuido': melhor_cluster_id,\n",
    "                'score_similaridade': maior_similaridade,\n",
    "                'status_influenciador': 1 if status_influencer else 0\n",
    "            })\n",
    "\n",
    "        # Salva as alterações nos arquivos\n",
    "        self._salvar_estado()\n",
    "        \n",
    "        # Une os resultados com o dataframe original para retorno\n",
    "        df_resultados = pd.DataFrame(resultados_finais)\n",
    "        return pd.merge(novos_dados, df_resultados, on='id')\n",
    "\n",
    "# --- EXEMPLO DE USO EM PRODUÇÃO ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detector = DetectorIncrementalInfluenciador()\n",
    "\n",
    "    # Se for a primeira vez rodando na vida:\n",
    "    if detector.vetorizador is None:\n",
    "        base_treino = [\n",
    "            \"O pix está fora do ar desde cedo\",\n",
    "            \"O pix está fora do ar desde cedo\",\n",
    "            \"O pix está fora do ar desde cedo\",\n",
    "            \"O pix está fora do ar desde cedo\",\n",
    "            \"O pix está fora do ar desde cedo\",\n",
    "            \"O pix está fora do ar desde cedo\",\n",
    "            \"Não consigo realizar transferencia via pix\",\n",
    "            \"Meu cartão de crédito ainda não chegou\", \n",
    "            \"Bom dia, gostaria de tirar uma duvida\",\n",
    "            \"O aplicativo é muito bonito parabens\", \n",
    "            \"O aplicativo é muito bonito\", \n",
    "            \"Alguém me ajuda com o pix por favor\",\n",
    "            \"Alguém me ajuda com o pix\",\n",
    "            \"me ajuda com o pix por favor\",\n",
    "        ]\n",
    "        detector.inicializar_modelo(base_treino)\n",
    "\n",
    "    # Simulação de dados chegando hoje\n",
    "    dados_hoje = pd.DataFrame({\n",
    "    'id': [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
    "    'texto': [\n",
    "        \"Erro generalizado no pix, arrumem logo\",\n",
    "        \"Pix travado aqui também\",\n",
    "        \"Estou aguardando a entrega do cartão a dias\",\n",
    "        \"Atendimento péssimo ninguem responde\",\n",
    "        \"Vocês são horriveis me atendam\", \n",
    "        \"Vou processar voces pelo atendimento ruim\",\n",
    "        \"O aplicativo é muito bonito\", \n",
    "        \"Alguém me ajuda com o pix por favor\",\n",
    "        \"Alguém me ajuda com o pix\",\n",
    "        \"me ajuda com o pix por favor\",\n",
    "        \"O pix está fora do ar desde cedo\"\n",
    "\n",
    "    ],\n",
    "    'cpf': ['666', '222', '777', '888', '897', '888', '234', '345', '667', '245', '756']\n",
    "    })\n",
    "\n",
    "    relatorio = detector.processar_lote_diario(dados_hoje)\n",
    "    print(\"\\n--- RESULTADO DO PROCESSAMENTO ---\")\n",
    "    print(relatorio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eee6c09e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_influ' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     protocolos_antigos \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 3. Filtrar apenas o que é NOVO (O seu \"Anti-Join\")\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m df_novos_casos \u001b[38;5;241m=\u001b[39m \u001b[43mdf_influ\u001b[49m[\u001b[38;5;241m~\u001b[39mdf_influ[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotocolo\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(protocolos_antigos)]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 4. Ajustar nomes de colunas para o Detector (ele espera 'id', 'texto', 'cpf')\u001b[39;00m\n\u001b[0;32m     17\u001b[0m df_novos_casos \u001b[38;5;241m=\u001b[39m df_novos_casos\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotocolo\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexto_cliente\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpf\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpf\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Certifique-se que a coluna CPF existe no seu df_influ\u001b[39;00m\n\u001b[0;32m     21\u001b[0m })\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_influ' is not defined"
     ]
    }
   ],
   "source": [
    "# --- SIMULAÇÃO DE CARGA DE DADOS REAIS ---\n",
    "\n",
    "# 1. Supondo que você carregou seus dados do banco/CSV\n",
    "# df_influ = pd.read_sql(\"SELECT protocolo, texto_cliente, cpf, data_abertura FROM tabela\", conexao)\n",
    "\n",
    "# 2. Carregar o histórico de protocolos já processados (para não repetir trabalho)\n",
    "caminho_processados = \"protocolos_finalizados.csv\"\n",
    "if os.path.exists(caminho_processados):\n",
    "    protocolos_antigos = pd.read_csv(caminho_processados)['protocolo'].tolist()\n",
    "else:\n",
    "    protocolos_antigos = []\n",
    "\n",
    "# 3. Filtrar apenas o que é NOVO (O seu \"Anti-Join\")\n",
    "df_novos_casos = df_influ[~df_influ['protocolo'].isin(protocolos_antigos)].copy()\n",
    "\n",
    "# 4. Ajustar nomes de colunas para o Detector (ele espera 'id', 'texto', 'cpf')\n",
    "df_novos_casos = df_novos_casos.rename(columns={\n",
    "    'protocolo': 'id',\n",
    "    'texto_cliente': 'texto',\n",
    "    'cpf': 'cpf' # Certifique-se que a coluna CPF existe no seu df_influ\n",
    "})\n",
    "\n",
    "if not df_novos_casos.empty:\n",
    "    detector = DetectorIncrementalInfluenciador()\n",
    "\n",
    "    # Caso seja a PRIMEIRA execução de todas (Cold Start)\n",
    "    if detector.vetorizador is None:\n",
    "        print(\"Iniciando primeira execução histórica...\")\n",
    "        detector.inicializar_modelo(df_novos_casos['texto'])\n",
    "\n",
    "    # Processamento Incremental\n",
    "    print(f\"Processando {len(df_novos_casos)} novos protocolos...\")\n",
    "    relatorio_final = detector.processar_lote_diario(df_novos_casos)\n",
    "\n",
    "    # 5. Salvar a lista de protocolos processados para o dia de amanhã\n",
    "    novos_ids = pd.DataFrame({'protocolo': df_novos_casos['id']})\n",
    "    novos_ids.to_csv(caminho_processados, mode='a', index=False, header=not os.path.exists(caminho_processados))\n",
    "    \n",
    "    print(\"Processamento concluído e salvo.\")\n",
    "else:\n",
    "    print(\"Não há novos dados para processar hoje.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcf0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> INICIANDO SISTEMA >>>\n",
      "--- Treinando Vectorizer com 9 textos iniciais ---\n",
      "\n",
      ">>> PROCESSANDO DIA 1 (Base Histórica)...\n",
      "                                        texto  cluster_id  is_influencer  \\\n",
      "0            O pix está fora do ar desde cedo           1          False   \n",
      "1  Não consigo realizar transferencia via pix           2          False   \n",
      "2      Meu cartão de crédito ainda não chegou           3          False   \n",
      "3       Bom dia, gostaria de tirar uma duvida           4          False   \n",
      "4        O aplicativo é muito bonito parabens           5          False   \n",
      "5                 O aplicativo é muito bonito           5          False   \n",
      "6         Alguém me ajuda com o pix por favor           6          False   \n",
      "7                   Alguém me ajuda com o pix           6          False   \n",
      "8                me ajuda com o pix por favor           6          False   \n",
      "\n",
      "         action_log  \n",
      "0       New Cluster  \n",
      "1       New Cluster  \n",
      "2       New Cluster  \n",
      "3       New Cluster  \n",
      "4       New Cluster  \n",
      "5  Existing Cluster  \n",
      "6       New Cluster  \n",
      "7  Existing Cluster  \n",
      "8  Existing Cluster  \n",
      "\n",
      ">>> PROCESSANDO DIA 2 (Novos Casos)...\n",
      "                                         texto  cluster_id  is_influencer  \\\n",
      "0       Erro generalizado no pix, arrumem logo           7          False   \n",
      "1                      Pix travado aqui também           7          False   \n",
      "2  Estou aguardando a entrega do cartão a dias           8          False   \n",
      "3         Atendimento péssimo ninguem responde           9          False   \n",
      "4               Vocês são horriveis me atendam          10          False   \n",
      "5    Vou processar voces pelo atendimento ruim          11          False   \n",
      "6                  O aplicativo é muito bonito           5           True   \n",
      "7          Alguém me ajuda com o pix por favor           6           True   \n",
      "8                    Alguém me ajuda com o pix           6           True   \n",
      "9                 me ajuda com o pix por favor           6           True   \n",
      "\n",
      "         action_log  \n",
      "0       New Cluster  \n",
      "1  Existing Cluster  \n",
      "2       New Cluster  \n",
      "3       New Cluster  \n",
      "4       New Cluster  \n",
      "5       New Cluster  \n",
      "6  Existing Cluster  \n",
      "7  Existing Cluster  \n",
      "8  Existing Cluster  \n",
      "9  Existing Cluster  \n",
      "\n",
      ">>> ESTADO FINAL CLUSTER 7 <<<\n",
      "CPFs únicos: {'666', '222'}\n",
      "É Influencer? False\n"
     ]
    }
   ],
   "source": [
    "# --- SIMULAÇÃO COM DADOS MAIS COMPLEXOS ---\n",
    "\n",
    "dados_dia_1 = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'texto': [\n",
    "        \"O pix está fora do ar desde cedo\",\n",
    "        \"Não consigo realizar transferencia via pix\",\n",
    "        \"Meu cartão de crédito ainda não chegou\", \n",
    "        \"Bom dia, gostaria de tirar uma duvida\",\n",
    "        \"O aplicativo é muito bonito parabens\", \n",
    "        \"O aplicativo é muito bonito\", \n",
    "        \"Alguém me ajuda com o pix por favor\",\n",
    "        \"Alguém me ajuda com o pix\",\n",
    "        \"me ajuda com o pix por favor\",\n",
    "    ],\n",
    "    'cpf': ['111', '222', '333', '444', '555', '111', '098', '356', '356']\n",
    "})\n",
    "\n",
    "dados_dia_2 = pd.DataFrame({\n",
    "    'id': [7, 8, 9, 10, 11, 12, 13, 14, 15, 16],\n",
    "    'texto': [\n",
    "        \"Erro generalizado no pix, arrumem logo\",\n",
    "        \"Pix travado aqui também\",\n",
    "        \"Estou aguardando a entrega do cartão a dias\",\n",
    "        \"Atendimento péssimo ninguem responde\",\n",
    "        \"Vocês são horriveis me atendam\", \n",
    "        \"Vou processar voces pelo atendimento ruim\",\n",
    "        \"O aplicativo é muito bonito\", \n",
    "        \"Alguém me ajuda com o pix por favor\",\n",
    "        \"Alguém me ajuda com o pix\",\n",
    "        \"me ajuda com o pix por favor\",\n",
    "    ],\n",
    "    'cpf': ['666', '222', '777', '888', '897', '888', '234', '345', '667', '245']\n",
    "})\n",
    "\n",
    "print(\">>> INICIANDO SISTEMA >>>\")\n",
    "detector = IncrementalInfluencerDetector(similarity_threshold=0.6)\n",
    "\n",
    "# Passo 1: Treinamento Inicial (Cold Start)\n",
    "# Usamos o dia 1 para 'aprender' o vocabulário\n",
    "texts_train = [TextPreprocessor.clean(t) for t in dados_dia_1['texto']]\n",
    "detector.fit_vectorizer(texts_train)\n",
    "\n",
    "# Passo 2: Processamento do Dia 1\n",
    "print(\"\\n>>> PROCESSANDO DIA 1 (Base Histórica)...\")\n",
    "resultado_d1 = detector.process_daily_batch(dados_dia_1)\n",
    "print(resultado_d1[['texto', 'cluster_id', 'is_influencer', 'action_log']])\n",
    "\n",
    "# Passo 3: Processamento do Dia 2 (Incremental)\n",
    "print(\"\\n>>> PROCESSANDO DIA 2 (Novos Casos)...\")\n",
    "# Note que o CPF 555 vai se juntar ao cluster do saque.\n",
    "# Como teremos CPFs 111, 333 e 555 no cluster, ele virará Influencer = True\n",
    "resultado_d2 = detector.process_daily_batch(dados_dia_2)\n",
    "print(resultado_d2[['texto', 'cluster_id', 'is_influencer', 'action_log']])\n",
    "\n",
    "# Verificar estado final do Cluster que virou Influencer\n",
    "cluster_saque_id = resultado_d2.iloc[0]['cluster_id']\n",
    "print(f\"\\n>>> ESTADO FINAL CLUSTER {cluster_saque_id} <<<\")\n",
    "print(f\"CPFs únicos: {detector.clusters[cluster_saque_id]['cpfs']}\")\n",
    "print(f\"É Influencer? {detector.clusters[cluster_saque_id]['is_influencer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cef40d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
