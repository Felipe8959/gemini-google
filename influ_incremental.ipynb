{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b7ed72",
   "metadata": {},
   "source": [
    "# üìë Documenta√ß√£o do Projeto: Detector Incremental de Influenciadores\n",
    "\n",
    "## 1. Introdu√ß√£o\n",
    "Este projeto implementa um algoritmo de detec√ß√£o de influenciadores baseado em similaridade textual. O diferencial desta solu√ß√£o √© a abordagem **incremental**, que resolve os problemas de escalabilidade (matriz quadr√°tica) e instabilidade de clusters que ocorrem em processamentos di√°rios repetitivos.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. A Dor do Neg√≥cio (Problema Resolvido)\n",
    "* **Problema Original:** O reprocessamento di√°rio de toda a base textual gerava um crescimento exponencial de processamento ($O(N^2)$) e mudava os n√∫meros dos clusters a cada execu√ß√£o.\n",
    "* **Solu√ß√£o:** Processamento apenas do \"delta\" (novos casos), encaixando-os em estruturas de dados persistentes (Clusters e Centroides) que mant√™m sua identidade ao longo do tempo.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Defini√ß√£o de \"Influenciador\"\n",
    "Seguindo a regra de neg√≥cio estabelecida, um registro √© marcado como **influenciador = 1** se pertencer a um grupo (cluster) que possua:\n",
    "1.  **Similaridade Textual:** $\\ge 60\\%$ de semelhan√ßa com o assunto do grupo.\n",
    "2.  **Densidade de CPFs:** No m√≠nimo **3 CPFs distintos** reclamando sobre o mesmo assunto.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Arquitetura T√©cnica\n",
    "\n",
    "### 4.1. Fluxo de Processamento Incremental\n",
    "O algoritmo n√£o compara texto contra texto. Ele compara o **texto novo** contra o **Centroide** (o \"resumo\" matem√°tico) de cada cluster existente.\n",
    "\n",
    "\n",
    "\n",
    "### 4.2. Estrutura de Persist√™ncia\n",
    "O sistema utiliza tr√™s arquivos locais para manter o estado entre as execu√ß√µes:\n",
    "\n",
    "* **`vetorizador.pkl`**: Armazena o modelo TF-IDF (vocabul√°rio e pesos).\n",
    "* **`centroides.pkl`**: Lista de vetores que representam a m√©dia de cada assunto.\n",
    "* **`base_clusters.csv`**: Tabela com metadados (`cluster_id`, `cpfs_unicos`, `quantidade_mensagens`).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Funcionamento do C√≥digo\n",
    "\n",
    "### Passo 1: Limpeza e Normaliza√ß√£o\n",
    "O texto √© tratado para remover ru√≠dos que prejudicam a similaridade:\n",
    "* Remo√ß√£o de acentos e caracteres especiais.\n",
    "* Convers√£o para min√∫sculas.\n",
    "* Filtragem de *Stopwords* em portugu√™s.\n",
    "\n",
    "### Passo 2: Vetoriza√ß√£o e Compara√ß√£o\n",
    "O texto √© convertido em um vetor via TF-IDF. O sistema calcula a **Similaridade de Cosseno** contra os centroides carregados do arquivo `centroides.pkl`.\n",
    "\n",
    "### Passo 3: Atribui√ß√£o e Evolu√ß√£o\n",
    "* **Se similaridade > 0.60:** O texto entra no cluster existente. O centroide desse cluster √© recalculado (M√©dia M√≥vel) para que o grupo \"aprenda\" com a nova varia√ß√£o de escrita.\n",
    "* **Se similaridade < 0.60:** Um novo cluster √© criado no arquivo CSV e o novo centroide √© adicionado ao arquivo PKL.\n",
    "\n",
    "### Passo 4: Atualiza√ß√£o da Regra\n",
    "O CPF do novo registro √© adicionado ao `set` (conjunto √∫nico) de CPFs daquele cluster. Se o contador atingir 3, a flag `e_influenciador` no CSV √© alterada para `True`.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Vantagens do Modelo Incremental\n",
    "\n",
    "1.  **Consist√™ncia de IDs:** Se o assunto \"Problema no Pix\" recebeu o `cluster_id: 10` hoje, ele continuar√° sendo o ID 10 para sempre.\n",
    "2.  **Performance:** O tempo de execu√ß√£o cresce de forma linear com os novos dados, e n√£o exponencialmente com a base toda.\n",
    "3.  **Mem√≥ria Otimizada:** Armazenamos apenas um vetor por cluster, em vez de um vetor para cada mensagem enviada na hist√≥ria da empresa.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Como Executar (Exemplo R√°pido)\n",
    "\n",
    "```python\n",
    "# Importe as fun√ß√µes do script\n",
    "from detector_influencers import processar_dados\n",
    "\n",
    "# Carregue seus novos dados do dia\n",
    "df_novos = pd.read_sql(\"SELECT id, texto, cpf FROM sua_tabela WHERE data = 'hoje'\", conexao)\n",
    "\n",
    "# Execute o processador\n",
    "# Ele ler√° automaticamente os arquivos .pkl e .csv da pasta\n",
    "resultado = processar_dados(df_novos)\n",
    "\n",
    "# Salve o resultado de volta no seu banco de dados\n",
    "resultado.to_sql(\"tabela_resultados\", conexao, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c279c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> PROCESSANDO DIA 1...\n",
      "   id                                       texto  cluster_atribuido  \\\n",
      "0   1            O pix est√° fora do ar desde cedo                  1   \n",
      "1   2  N√£o consigo realizar transferencia via pix                 27   \n",
      "2   3      Meu cart√£o de cr√©dito ainda n√£o chegou                 28   \n",
      "3   4       Bom dia, gostaria de tirar uma duvida                 29   \n",
      "4   5        O aplicativo √© muito bonito parabens                 30   \n",
      "5   6                 O aplicativo √© muito bonito                 31   \n",
      "6   7         Algu√©m me ajuda com o pix por favor                  4   \n",
      "7   8                   Algu√©m me ajuda com o pix                  4   \n",
      "8   9                me ajuda com o pix por favor                  4   \n",
      "\n",
      "   influenciador  \n",
      "0              0  \n",
      "1              0  \n",
      "2              0  \n",
      "3              0  \n",
      "4              0  \n",
      "5              0  \n",
      "6              0  \n",
      "7              1  \n",
      "8              1  \n",
      "\n",
      ">>> PROCESSANDO DIA 2 (INCREMENTAL)...\n",
      "   id                                        texto  cluster_atribuido  \\\n",
      "0   7       Erro generalizado no pix, arrumem logo                  4   \n",
      "1   8                      Pix travado aqui tamb√©m                  4   \n",
      "2   9  Estou aguardando a entrega do cart√£o a dias                 32   \n",
      "3  10         Atendimento p√©ssimo ninguem responde                 33   \n",
      "4  11               Voc√™s s√£o horriveis me atendam                 34   \n",
      "5  12    Vou processar voces pelo atendimento ruim                 35   \n",
      "6  13                  O aplicativo √© muito bonito                 36   \n",
      "7  14          Algu√©m me ajuda com o pix por favor                  4   \n",
      "8  15                    Algu√©m me ajuda com o pix                  4   \n",
      "9  16                 me ajuda com o pix por favor                  4   \n",
      "\n",
      "   influenciador  \n",
      "0              1  \n",
      "1              1  \n",
      "2              0  \n",
      "3              0  \n",
      "4              0  \n",
      "5              0  \n",
      "6              0  \n",
      "7              1  \n",
      "8              1  \n",
      "9              1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- CONFIGURA√á√ïES DE CAMINHO ---\n",
    "ARQUIVO_MODELO = \"vetorizador.pkl\"\n",
    "ARQUIVO_CLUSTERS = \"base_clusters.csv\"\n",
    "ARQUIVO_CENTROIDES = \"centroides.pkl\"\n",
    "\n",
    "# --- 1. FUN√á√ÉO DE LIMPEZA (O seu tratamento de texto) ---\n",
    "def limpar_texto(texto):\n",
    "    \"\"\"Remove caracteres especiais, acentos e padroniza para min√∫sculo.\"\"\"\n",
    "    if not isinstance(texto, str): return \"\"\n",
    "    texto = texto.lower()\n",
    "    # Mant√©m apenas letras de 'a' a 'z' e espa√ßos\n",
    "    texto = re.sub(r'[^a-z\\s]', '', texto)\n",
    "    return texto.strip()\n",
    "\n",
    "# --- 2. FUN√á√ïES DE CARGA E SALVAMENTO ---\n",
    "def carregar_estado():\n",
    "    \"\"\"Carrega os dados salvos anteriormente para continuar de onde parou.\"\"\"\n",
    "    vetorizador = None\n",
    "    clusters_df = pd.DataFrame()\n",
    "    centroides_lista = []\n",
    "\n",
    "    if os.path.exists(ARQUIVO_MODELO):\n",
    "        with open(ARQUIVO_MODELO, 'rb') as f:\n",
    "            vetorizador = pickle.load(f)\n",
    "    \n",
    "    if os.path.exists(ARQUIVO_CLUSTERS):\n",
    "        clusters_df = pd.read_csv(ARQUIVO_CLUSTERS)\n",
    "        # Converte o texto da coluna 'cpfs_unicos' de volta para um conjunto (set)\n",
    "        clusters_df['cpfs_unicos'] = clusters_df['cpfs_unicos'].apply(eval).apply(set)\n",
    "    \n",
    "    if os.path.exists(ARQUIVO_CENTROIDES):\n",
    "        with open(ARQUIVO_CENTROIDES, 'rb') as f:\n",
    "            centroides_lista = pickle.load(f)\n",
    "            \n",
    "    return vetorizador, clusters_df, centroides_lista\n",
    "\n",
    "def salvar_estado(vetorizador, clusters_df, centroides_lista):\n",
    "    \"\"\"Salva os dados atuais em arquivos para persist√™ncia.\"\"\"\n",
    "    with open(ARQUIVO_MODELO, 'wb') as f:\n",
    "        pickle.dump(vetorizador, f)\n",
    "    \n",
    "    with open(ARQUIVO_CENTROIDES, 'wb') as f:\n",
    "        pickle.dump(centroides_lista, f)\n",
    "        \n",
    "    # No CSV, salvamos o set de CPFs como lista para ser leg√≠vel\n",
    "    df_temp = clusters_df.copy()\n",
    "    df_temp['cpfs_unicos'] = df_temp['cpfs_unicos'].apply(list)\n",
    "    df_temp.to_csv(ARQUIVO_CLUSTERS, index=False)\n",
    "\n",
    "# --- 3. L√ìGICA PRINCIPAL (INCREMENTAL) ---\n",
    "def processar_dados(df_novo, limiar=0.60, min_cpfs=3):\n",
    "    # Tenta carregar o que j√° existe\n",
    "    vetorizador, clusters_df, centroides_lista = carregar_estado()\n",
    "\n",
    "    # Se o vetorizador n√£o existir, cria um novo (Treino inicial)\n",
    "    if vetorizador is None:\n",
    "        print(\"Treinando vetorizador pela primeira vez...\")\n",
    "        try:\n",
    "            stops = stopwords.words('portuguese')\n",
    "        except:\n",
    "            nltk.download('stopwords')\n",
    "            stops = stopwords.words('portuguese')\n",
    "        \n",
    "        vetorizador = TfidfVectorizer(stop_words=stops, max_features=5000)\n",
    "        textos_treino = [limpar_texto(t) for t in df_novo['texto']]\n",
    "        vetorizador.fit(textos_treino)\n",
    "\n",
    "    # Prepara os textos novos\n",
    "    textos_limpos = [limpar_texto(t) for t in df_novo['texto']]\n",
    "    matriz_novos_vetores = vetorizador.transform(textos_limpos)\n",
    "    \n",
    "    resultados = []\n",
    "\n",
    "    # Percorre cada texto novo\n",
    "    for i in range(matriz_novos_vetores.shape[0]):\n",
    "        vetor_atual = matriz_novos_vetores[i]\n",
    "        cpf_atual = str(df_novo.iloc[i]['cpf'])\n",
    "        id_atual = df_novo.iloc[i]['id']\n",
    "        \n",
    "        melhor_cluster_idx = -1\n",
    "        maior_score = 0.0\n",
    "\n",
    "        # Compara com os centroides existentes\n",
    "        if len(centroides_lista) > 0:\n",
    "            matriz_centroides = np.vstack(centroides_lista)\n",
    "            sims = cosine_similarity(vetor_atual, matriz_centroides).flatten()\n",
    "            melhor_cluster_idx = np.argmax(sims)\n",
    "            maior_score = sims[melhor_cluster_idx]\n",
    "\n",
    "        # SE encontrar similaridade >= 60%\n",
    "        if melhor_cluster_idx != -1 and maior_score >= limiar:\n",
    "            # Pega o ID do cluster encontrado\n",
    "            c_id = clusters_df.iloc[melhor_cluster_idx]['cluster_id']\n",
    "            \n",
    "            # Atualiza metadados (CPFs e Quantidade)\n",
    "            clusters_df.at[melhor_cluster_idx, 'cpfs_unicos'].add(cpf_atual)\n",
    "            clusters_df.at[melhor_cluster_idx, 'quantidade_mensagens'] += 1\n",
    "            \n",
    "            # Verifica se virou influenciador\n",
    "            if len(clusters_df.at[melhor_cluster_idx, 'cpfs_unicos']) >= min_cpfs:\n",
    "                clusters_df.at[melhor_cluster_idx, 'e_influenciador'] = True\n",
    "            \n",
    "            # Recalcula o centroide (M√©dia m√≥vel para o grupo evoluir)\n",
    "            n = clusters_df.at[melhor_cluster_idx, 'quantidade_mensagens']\n",
    "            antigo_centroide = centroides_lista[melhor_cluster_idx]\n",
    "            novo_centroide = (antigo_centroide * (n-1) + vetor_atual.toarray()) / n\n",
    "            centroides_lista[melhor_cluster_idx] = novo_centroide\n",
    "            \n",
    "            cluster_final = c_id\n",
    "        else:\n",
    "            # SE N√ÉO encontrar, cria um cluster NOVO\n",
    "            cluster_final = len(clusters_df) + 1\n",
    "            novo_reg = {\n",
    "                'cluster_id': cluster_final,\n",
    "                'quantidade_mensagens': 1,\n",
    "                'cpfs_unicos': {cpf_atual},\n",
    "                'e_influenciador': False\n",
    "            }\n",
    "            clusters_df = pd.concat([clusters_df, pd.DataFrame([novo_reg])], ignore_index=True)\n",
    "            centroides_lista.append(vetor_atual.toarray())\n",
    "            maior_score = 1.0 # Similaridade com ele mesmo ao criar\n",
    "\n",
    "        # Adiciona ao relat√≥rio de sa√≠da\n",
    "        e_influ = clusters_df[clusters_df['cluster_id'] == cluster_final]['e_influenciador'].values[0]\n",
    "        resultados.append({\n",
    "            'id': id_atual,\n",
    "            'cluster_atribuido': cluster_final,\n",
    "            'similaridade': round(float(maior_score), 4),\n",
    "            'influenciador': 1 if e_influ else 0\n",
    "        })\n",
    "\n",
    "    # Salva tudo nos arquivos para a pr√≥xima rodada\n",
    "    salvar_estado(vetorizador, clusters_df, centroides_lista)\n",
    "    \n",
    "    # Retorna o DataFrame com as novas colunas\n",
    "    return pd.merge(df_novo, pd.DataFrame(resultados), on='id')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- DIA 1: Execu√ß√£o Inicial ---\n",
    "    print(\">>> PROCESSANDO DIA 1...\")\n",
    "    dados_dia_1 = pd.DataFrame({\n",
    "        'id': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "        'texto': [\n",
    "            \"O pix est√° fora do ar desde cedo\",\n",
    "            \"N√£o consigo realizar transferencia via pix\",\n",
    "            \"Meu cart√£o de cr√©dito ainda n√£o chegou\", \n",
    "            \"Bom dia, gostaria de tirar uma duvida\",\n",
    "            \"O aplicativo √© muito bonito parabens\", \n",
    "            \"O aplicativo √© muito bonito\", \n",
    "            \"Algu√©m me ajuda com o pix por favor\",\n",
    "            \"Algu√©m me ajuda com o pix\",\n",
    "            \"me ajuda com o pix por favor\",\n",
    "        ],\n",
    "        'cpf': ['111', '222', '333', '444', '555', '111', '098', '356', '356']\n",
    "    })\n",
    "    \n",
    "    # O sistema cria os arquivos e identifica os primeiros clusters\n",
    "    resultado_d1 = processar_dados(dados_dia_1)\n",
    "    print(resultado_d1[['id', 'texto', 'cluster_atribuido', 'influenciador']])\n",
    "\n",
    "\n",
    "    # --- DIA 2: Novos dados chegando ---\n",
    "    print(\"\\n>>> PROCESSANDO DIA 2 (INCREMENTAL)...\")\n",
    "    dados_dia_2 = pd.DataFrame({\n",
    "        'id': [7, 8, 9, 10, 11, 12, 13, 14, 15, 16],\n",
    "        'texto': [\n",
    "            \"Erro generalizado no pix, arrumem logo\",\n",
    "            \"Pix travado aqui tamb√©m\",\n",
    "            \"Estou aguardando a entrega do cart√£o a dias\",\n",
    "            \"Atendimento p√©ssimo ninguem responde\",\n",
    "            \"Voc√™s s√£o horriveis me atendam\", \n",
    "            \"Vou processar voces pelo atendimento ruim\",\n",
    "            \"O aplicativo √© muito bonito\", \n",
    "            \"Algu√©m me ajuda com o pix por favor\",\n",
    "            \"Algu√©m me ajuda com o pix\",\n",
    "            \"me ajuda com o pix por favor\",\n",
    "        ],\n",
    "        'cpf': ['666', '222', '777', '888', '897', '888', '234', '345', '667', '245']\n",
    "    })\n",
    "\n",
    "    # Aqui a m√°gica acontece: ele carrega os arquivos do Dia 1 automaticamente\n",
    "    resultado_d2 = processar_dados(dados_dia_2)\n",
    "    print(resultado_d2[['id', 'texto', 'cluster_atribuido', 'influenciador']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa675c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURA√á√ïES T√âCNICAS E ARQUIVOS DE MEM√ìRIA\n",
    "# =============================================================================\n",
    "# O sistema utiliza esses arquivos para \"lembrar\" o que j√° processou.\n",
    "ARQUIVO_MODELO = \"vetorizador_tfidf.pkl\"      # Guarda o vocabul√°rio aprendido\n",
    "ARQUIVO_CLUSTERS = \"base_clusters_metadados.csv\" # Guarda IDs, CPFs e flags\n",
    "ARQUIVO_CENTROIDES = \"matriz_centroides.pkl\"   # Guarda a ess√™ncia matem√°tica de cada grupo\n",
    "\n",
    "# =============================================================================\n",
    "# ETAPA 1: TRATAMENTO DE TEXTO (LIMPANDO O RU√çDO)\n",
    "# =============================================================================\n",
    "def limpar_texto(texto):\n",
    "    \"\"\"\n",
    "    Padroniza o texto para que 'Pix' e 'pix!!!' sejam lidos da mesma forma.\n",
    "    \"\"\"\n",
    "    if not isinstance(texto, str): \n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Converte para min√∫sculas\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    # 2. Remove acentos (opcional, aqui simplificado via Regex)\n",
    "    # 3. Remove caracteres especiais e n√∫meros, mantendo apenas letras e espa√ßos\n",
    "    texto = re.sub(r'[^a-z\\s]', '', texto)\n",
    "    \n",
    "    # 4. Remove espa√ßos extras no in√≠cio e fim\n",
    "    return texto.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# ETAPA 2: PERSIST√äNCIA DE DADOS (CARGA E SALVAMENTO)\n",
    "# =============================================================================\n",
    "def carregar_estado_anterior():\n",
    "    \"\"\"Tenta carregar os arquivos da pasta local. Se n√£o existirem, retorna vazio.\"\"\"\n",
    "    vetorizador = None\n",
    "    clusters_df = pd.DataFrame()\n",
    "    centroides_lista = []\n",
    "\n",
    "    print(\"\\n--- [SISTEMA] Verificando arquivos de mem√≥ria hist√≥rica ---\")\n",
    "    \n",
    "    if os.path.exists(ARQUIVO_MODELO):\n",
    "        with open(ARQUIVO_MODELO, 'rb') as f:\n",
    "            vetorizador = pickle.load(f)\n",
    "        print(\">> Vetorizador TF-IDF carregado com sucesso.\")\n",
    "    \n",
    "    if os.path.exists(ARQUIVO_CLUSTERS):\n",
    "        clusters_df = pd.read_csv(ARQUIVO_CLUSTERS)\n",
    "        # O CSV salva o conjunto de CPFs como String. Precisamos converter de volta para 'set'.\n",
    "        clusters_df['cpfs_unicos'] = clusters_df['cpfs_unicos'].apply(eval).apply(set)\n",
    "        print(f\">> Base de Clusters carregada: {len(clusters_df)} grupos conhecidos.\")\n",
    "    \n",
    "    if os.path.exists(ARQUIVO_CENTROIDES):\n",
    "        with open(ARQUIVO_CENTROIDES, 'rb') as f:\n",
    "            centroides_lista = pickle.load(f)\n",
    "        print(\">> Matriz de Centroides carregada.\")\n",
    "            \n",
    "    return vetorizador, clusters_df, centroides_lista\n",
    "\n",
    "def salvar_estado_atual(vetorizador, clusters_df, centroides_lista):\n",
    "    \"\"\"Grava as atualiza√ß√µes nos arquivos locais para a pr√≥xima execu√ß√£o.\"\"\"\n",
    "    print(\"\\n--- [SISTEMA] Salvando progresso nos arquivos locais ---\")\n",
    "    \n",
    "    with open(ARQUIVO_MODELO, 'wb') as f:\n",
    "        pickle.dump(vetorizador, f)\n",
    "    \n",
    "    with open(ARQUIVO_CENTROIDES, 'wb') as f:\n",
    "        pickle.dump(centroides_lista, f)\n",
    "        \n",
    "    # Converte o set de CPFs em lista para o CSV ser leg√≠vel por humanos/Excel\n",
    "    df_para_disco = clusters_df.copy()\n",
    "    df_para_disco['cpfs_unicos'] = df_para_disco['cpfs_unicos'].apply(list)\n",
    "    df_para_disco.to_csv(ARQUIVO_CLUSTERS, index=False)\n",
    "    print(\">> Tudo salvo com sucesso. Sistema pronto para a pr√≥xima rodada.\")\n",
    "\n",
    "# =============================================================================\n",
    "# ETAPA 3: PROCESSAMENTO INCREMENTAL (O CORA√á√ÉO DO ALGORITMO)\n",
    "# =============================================================================\n",
    "def processar_dados_incrementais(df_entrada, limiar_similaridade=0.60, min_cpfs_influencer=3):\n",
    "    \"\"\"\n",
    "    Recebe novos dados e decide se encaixa em grupos velhos ou cria novos.\n",
    "    \"\"\"\n",
    "    # 1. Carrega o que j√° conhecemos de ontem\n",
    "    vetorizador, clusters_df, centroides_lista = carregar_estado_anterior()\n",
    "\n",
    "    # 2. Inicializa√ß√£o do Modelo (se for a primeira execu√ß√£o da hist√≥ria)\n",
    "    if vetorizador is None:\n",
    "        print(\"\\n--- [TESTE] Modelo n√£o encontrado. Iniciando Treinamento Inicial ---\")\n",
    "        try:\n",
    "            stops = stopwords.words('portuguese')\n",
    "        except:\n",
    "            nltk.download('stopwords')\n",
    "            stops = stopwords.words('portuguese')\n",
    "        \n",
    "        vetorizador = TfidfVectorizer(stop_words=stops, max_features=5000)\n",
    "        textos_iniciais = [limpar_texto(t) for t in df_entrada['texto']]\n",
    "        vetorizador.fit(textos_iniciais)\n",
    "        print(\">> Treinamento do vocabul√°rio conclu√≠do.\")\n",
    "\n",
    "    # 3. Vetoriza√ß√£o do lote atual\n",
    "    print(f\"\\n--- [TESTE] Vetorizando {len(df_entrada)} novas mensagens ---\")\n",
    "    textos_limpos = [limpar_texto(t) for t in df_entrada['texto']]\n",
    "    matriz_atual = vetorizador.transform(textos_limpos)\n",
    "    \n",
    "    registros_processados = []\n",
    "\n",
    "    # 4. Loop linha por linha para garantir o processamento incremental puro\n",
    "    for i in range(matriz_atual.shape[0]):\n",
    "        vetor_da_mensagem = matriz_atual[i]\n",
    "        cpf = str(df_entrada.iloc[i]['cpf'])\n",
    "        id_msg = df_entrada.iloc[i]['id']\n",
    "        txt_original = df_entrada.iloc[i]['texto']\n",
    "        \n",
    "        melhor_match_idx = -1\n",
    "        maior_score = 0.0\n",
    "\n",
    "        # COMPARANDO COM O PASSADO: Buscamos nos centroides j√° existentes\n",
    "        if len(centroides_lista) > 0:\n",
    "            matriz_centroides = np.vstack(centroides_lista)\n",
    "            sims = cosine_similarity(vetor_da_mensagem, matriz_centroides).flatten()\n",
    "            melhor_match_idx = np.argmax(sims)\n",
    "            maior_score = sims[melhor_match_idx]\n",
    "\n",
    "        # DECIS√ÉO: Encaixar ou Criar?\n",
    "        if melhor_match_idx != -1 and maior_score >= limiar_similaridade:\n",
    "            # ENCAIXE: O texto √© parecido com um grupo existente\n",
    "            cluster_id_destino = clusters_df.iloc[melhor_match_idx]['cluster_id']\n",
    "            \n",
    "            # Atualiza os metadados do grupo\n",
    "            clusters_df.at[melhor_match_idx, 'cpfs_unicos'].add(cpf)\n",
    "            clusters_df.at[melhor_match_idx, 'quantidade_mensagens'] += 1\n",
    "            \n",
    "            # Verifica se atingiu a regra de Influencer (3 CPFs)\n",
    "            if len(clusters_df.at[melhor_match_idx, 'cpfs_unicos']) >= min_cpfs_influencer:\n",
    "                clusters_df.at[melhor_match_idx, 'e_influenciador'] = True\n",
    "            \n",
    "            # EVOLU√á√ÉO: O centroide do grupo muda um pouco para se adaptar √† nova escrita\n",
    "            # F√≥rmula da M√©dia M√≥vel: (M√©dia_Antiga * N + Novo_Valor) / (N+1)\n",
    "            n_mensagens = clusters_df.at[melhor_match_idx, 'quantidade_mensagens']\n",
    "            centroide_antigo = centroides_lista[melhor_match_idx]\n",
    "            novo_centroide = (centroide_antigo * (n_mensagens - 1) + vetor_da_mensagem.toarray()) / n_mensagens\n",
    "            centroides_lista[melhor_match_idx] = novo_centroide\n",
    "            \n",
    "            print(f\"[TESTE] Msg ID {id_msg}: Alocada ao Cluster {cluster_id_destino} (Score: {maior_score:.2f})\")\n",
    "        else:\n",
    "            # CRIA√á√ÉO: Assunto novo detectado\n",
    "            novo_id = len(clusters_df) + 1\n",
    "            novo_registro_cluster = {\n",
    "                'cluster_id': novo_id,\n",
    "                'quantidade_mensagens': 1,\n",
    "                'cpfs_unicos': {cpf},\n",
    "                'e_influenciador': False\n",
    "            }\n",
    "            clusters_df = pd.concat([clusters_df, pd.DataFrame([novo_registro_cluster])], ignore_index=True)\n",
    "            centroides_lista.append(vetor_da_mensagem.toarray())\n",
    "            cluster_id_destino = novo_id\n",
    "            print(f\"[TESTE] Msg ID {id_msg}: Novo Assunto detectado! Cluster {novo_id} criado.\")\n",
    "\n",
    "        # Coleta o status de influencer atualizado para o relat√≥rio final\n",
    "        status_influ = clusters_df[clusters_df['cluster_id'] == cluster_id_destino]['e_influenciador'].values[0]\n",
    "        \n",
    "        registros_processados.append({\n",
    "            'id': id_msg,\n",
    "            'cluster_atribuido': cluster_id_destino,\n",
    "            'similaridade': maior_score,\n",
    "            'influenciador': 1 if status_influ else 0\n",
    "        })\n",
    "\n",
    "    # 5. Persiste as mudan√ßas\n",
    "    salvar_estado_atual(vetorizador, clusters_df, centroides_lista)\n",
    "    \n",
    "    # 6. Retorna o DF original enriquecido\n",
    "    df_saida = pd.merge(df_entrada, pd.DataFrame(registros_processados), on='id')\n",
    "    return df_saida\n",
    "\n",
    "# =============================================================================\n",
    "# ETAPA 4: TESTE DE EXECU√á√ÉO (SIMULANDO DOIS DIAS)\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # DIA 1\n",
    "    print(\"=\"*60)\n",
    "    print(\"SIMULA√á√ÉO - DIA 1\")\n",
    "    print(\"=\"*60)\n",
    "    df_dia1 = pd.DataFrame({\n",
    "        'id': [1, 2, 3],\n",
    "        'texto': [\"Problema no pix hoje\", \"Meu pix nao funciona\", \"Atraso no cartao\"],\n",
    "        'cpf': ['CPF_A', 'CPF_B', 'CPF_C']\n",
    "    })\n",
    "    relatorio_d1 = processar_dados_incrementais(df_dia1)\n",
    "    print(\"\\n--- RELAT√ìRIO FINAL DIA 1 ---\")\n",
    "    print(relatorio_d1[['id', 'texto', 'cluster_atribuido', 'influenciador']])\n",
    "\n",
    "    # DIA 2 (Aqui o sistema deve reconhecer o cluster do PIX)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SIMULA√á√ÉO - DIA 2 (INCREMENTAL)\")\n",
    "    print(\"=\"*60)\n",
    "    df_dia2 = pd.DataFrame({\n",
    "        'id': [4, 5],\n",
    "        'texto': [\"Erro no sistema de pix\", \"Nao recebi meu cartao ainda\"],\n",
    "        'cpf': ['CPF_D', 'CPF_E']\n",
    "    })\n",
    "    relatorio_d2 = processar_dados_incrementais(df_dia2)\n",
    "    print(\"\\n--- RELAT√ìRIO FINAL DIA 2 ---\")\n",
    "    print(relatorio_d2[['id', 'texto', 'cluster_atribuido', 'influenciador']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8422e948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULTADO DO PROCESSAMENTO ---\n",
      "    id                                        texto  cpf  cluster_atribuido  \\\n",
      "0    7       Erro generalizado no pix, arrumem logo  666                 13   \n",
      "1    8                      Pix travado aqui tamb√©m  222                 14   \n",
      "2    9  Estou aguardando a entrega do cart√£o a dias  777                 15   \n",
      "3   10         Atendimento p√©ssimo ninguem responde  888                 16   \n",
      "4   11               Voc√™s s√£o horriveis me atendam  897                 17   \n",
      "5   12    Vou processar voces pelo atendimento ruim  888                 18   \n",
      "6   13                  O aplicativo √© muito bonito  234                 19   \n",
      "7   14          Algu√©m me ajuda com o pix por favor  345                 20   \n",
      "8   15                    Algu√©m me ajuda com o pix  667                 21   \n",
      "9   16                 me ajuda com o pix por favor  245                 22   \n",
      "10  17             O pix est√° fora do ar desde cedo  756                 23   \n",
      "\n",
      "    score_similaridade  status_influenciador  \n",
      "0                  0.0                     0  \n",
      "1                  0.0                     0  \n",
      "2                  0.0                     0  \n",
      "3                  0.0                     0  \n",
      "4                  0.0                     0  \n",
      "5                  0.0                     0  \n",
      "6                  0.0                     0  \n",
      "7                  0.0                     0  \n",
      "8                  0.0                     0  \n",
      "9                  0.0                     0  \n",
      "10                 0.0                     0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- CONFIGURA√á√ïES DE ARQUIVOS ---\n",
    "CAMINHO_MODELO = \"vetorizador_tfidf.pkl\"\n",
    "CAMINHO_CLUSTERS = \"base_clusters.csv\"\n",
    "CAMINHO_CENTROIDES = \"centroides_matriz.pkl\"\n",
    "\n",
    "class ProcessadorTexto:\n",
    "    \"\"\"Classe respons√°vel pela limpeza padronizada dos textos\"\"\"\n",
    "    @staticmethod\n",
    "    def limpar(texto):\n",
    "        if not isinstance(texto, str): return \"\"\n",
    "        texto = texto.lower()\n",
    "        # Remove acentos e caracteres especiais\n",
    "        texto = re.sub(r'[^a-z\\s]', '', texto)\n",
    "        return texto.strip()\n",
    "\n",
    "class DetectorIncrementalInfluenciador:\n",
    "    def __init__(self, similaridade_minima=0.60, minimo_cpfs=3):\n",
    "        self.limiar = similaridade_minima\n",
    "        self.min_cpfs = minimo_cpfs\n",
    "        self.vetorizador = None\n",
    "        self.clusters_df = pd.DataFrame() # Mem√≥ria dos metadados dos clusters\n",
    "        self.centroides_lista = []        # Mem√≥ria dos vetores (centroides)\n",
    "        \n",
    "        # Tenta carregar dados existentes ao iniciar\n",
    "        self._carregar_estado()\n",
    "\n",
    "    def _carregar_estado(self):\n",
    "        \"\"\"Carrega os arquivos salvos se eles existirem\"\"\"\n",
    "        if os.path.exists(CAMINHO_MODELO):\n",
    "            with open(CAMINHO_MODELO, 'rb') as f:\n",
    "                self.vetorizador = pickle.load(f)\n",
    "        \n",
    "        if os.path.exists(CAMINHO_CLUSTERS):\n",
    "            self.clusters_df = pd.read_csv(CAMINHO_CLUSTERS)\n",
    "            # Converte a string de CPFs de volta para um conjunto (set)\n",
    "            self.clusters_df['cpfs_unicos'] = self.clusters_df['cpfs_unicos'].apply(eval).apply(set)\n",
    "        \n",
    "        if os.path.exists(CAMINHO_CENTROIDES):\n",
    "            with open(CAMINHO_CENTROIDES, 'rb') as f:\n",
    "                self.centroides_lista = pickle.load(f)\n",
    "\n",
    "    def _salvar_estado(self):\n",
    "        \"\"\"Persiste os dados em disco para a pr√≥xima execu√ß√£o\"\"\"\n",
    "        with open(CAMINHO_MODELO, 'wb') as f:\n",
    "            pickle.dump(self.vetorizador, f)\n",
    "        \n",
    "        with open(CAMINHO_CENTROIDES, 'wb') as f:\n",
    "            pickle.dump(self.centroides_lista, f)\n",
    "            \n",
    "        # Para salvar no CSV, convertemos o set de CPFs em lista (string)\n",
    "        df_para_salvar = self.clusters_df.copy()\n",
    "        df_para_salvar['cpfs_unicos'] = df_para_salvar['cpfs_unicos'].apply(list)\n",
    "        df_para_salvar.to_csv(CAMINHO_CLUSTERS, index=False)\n",
    "\n",
    "    def inicializar_modelo(self, textos_treino):\n",
    "        \"\"\"Treina o TF-IDF pela primeira vez (Cold Start)\"\"\"\n",
    "        from nltk.corpus import stopwords\n",
    "        import nltk\n",
    "        try:\n",
    "            pt_stops = stopwords.words('portuguese')\n",
    "        except:\n",
    "            nltk.download('stopwords')\n",
    "            pt_stops = stopwords.words('portuguese')\n",
    "\n",
    "        self.vetorizador = TfidfVectorizer(stop_words=pt_stops, max_features=5000)\n",
    "        textos_limpos = [ProcessadorTexto.limpar(t) for t in textos_treino]\n",
    "        self.vetorizador.fit(textos_limpos)\n",
    "        print(\"‚úÖ Modelo TF-IDF inicializado e treinado.\")\n",
    "\n",
    "    def processar_lote_diario(self, novos_dados):\n",
    "        \"\"\"\n",
    "        Recebe um DataFrame com ['id', 'texto', 'cpf'].\n",
    "        Retorna o mesmo DataFrame com as colunas de cluster e influencer.\n",
    "        \"\"\"\n",
    "        textos_limpos = [ProcessadorTexto.limpar(t) for t in novos_dados['texto']]\n",
    "        matriz_vetores = self.vetorizador.transform(textos_limpos)\n",
    "        \n",
    "        resultados_finais = []\n",
    "\n",
    "        # Processa cada linha do novo lote\n",
    "        for i in range(matriz_vetores.shape[0]):\n",
    "            vetor_atual = matriz_vetores[i]\n",
    "            cpf_atual = str(novos_dados.iloc[i]['cpf'])\n",
    "            id_origem = novos_dados.iloc[i]['id']\n",
    "            \n",
    "            melhor_cluster_id = None\n",
    "            maior_similaridade = 0.0\n",
    "            \n",
    "            # 1. Compara√ß√£o com clusters existentes\n",
    "            if len(self.centroides_lista) > 0:\n",
    "                # Transforma lista de centroides em matriz para c√°lculo r√°pido\n",
    "                matriz_centroides = np.vstack(self.centroides_lista)\n",
    "                similaridades = cosine_similarity(vetor_atual, matriz_centroides).flatten()\n",
    "                \n",
    "                indice_melhor = np.argmax(similaridades)\n",
    "                if similaridades[indice_melhor] >= self.limiar:\n",
    "                    maior_similaridade = similaridades[indice_melhor]\n",
    "                    melhor_cluster_id = self.clusters_df.iloc[indice_melhor]['cluster_id']\n",
    "\n",
    "            # 2. L√≥gica de Atribui√ß√£o ou Cria√ß√£o\n",
    "            if melhor_cluster_id is not None:\n",
    "                # Atualiza cluster existente\n",
    "                idx = self.clusters_df[self.clusters_df['cluster_id'] == melhor_cluster_id].index[0]\n",
    "                \n",
    "                # Atualiza CPFs e contagem\n",
    "                self.clusters_df.at[idx, 'cpfs_unicos'].add(cpf_atual)\n",
    "                total_cpfs = len(self.clusters_df.at[idx, 'cpfs_unicos'])\n",
    "                self.clusters_df.at[idx, 'quantidade_mensagens'] += 1\n",
    "                \n",
    "                # Verifica regra de Influencer\n",
    "                if total_cpfs >= self.min_cpfs:\n",
    "                    self.clusters_df.at[idx, 'e_influenciador'] = True\n",
    "                \n",
    "                # Atualiza o centroide (m√©dia m√≥vel)\n",
    "                n = self.clusters_df.at[idx, 'quantidade_mensagens']\n",
    "                novo_centroide = (self.centroides_lista[idx] * (n-1) + vetor_atual.toarray()) / n\n",
    "                self.centroides_lista[idx] = novo_centroide\n",
    "            else:\n",
    "                # Cria novo cluster\n",
    "                novo_id = len(self.clusters_df) + 1\n",
    "                novo_registro = {\n",
    "                    'cluster_id': novo_id,\n",
    "                    'quantidade_mensagens': 1,\n",
    "                    'cpfs_unicos': {cpf_atual},\n",
    "                    'e_influenciador': False\n",
    "                }\n",
    "                self.clusters_df = pd.concat([self.clusters_df, pd.DataFrame([novo_registro])], ignore_index=True)\n",
    "                self.centroides_lista.append(vetor_atual.toarray())\n",
    "                melhor_cluster_id = novo_id\n",
    "\n",
    "            # Guardar resultado da linha atual\n",
    "            status_influencer = self.clusters_df[self.clusters_df['cluster_id'] == melhor_cluster_id]['e_influenciador'].values[0]\n",
    "            resultados_finais.append({\n",
    "                'id': id_origem,\n",
    "                'cluster_atribuido': melhor_cluster_id,\n",
    "                'score_similaridade': maior_similaridade,\n",
    "                'status_influenciador': 1 if status_influencer else 0\n",
    "            })\n",
    "\n",
    "        # Salva as altera√ß√µes nos arquivos\n",
    "        self._salvar_estado()\n",
    "        \n",
    "        # Une os resultados com o dataframe original para retorno\n",
    "        df_resultados = pd.DataFrame(resultados_finais)\n",
    "        return pd.merge(novos_dados, df_resultados, on='id')\n",
    "\n",
    "# --- EXEMPLO DE USO EM PRODU√á√ÉO ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detector = DetectorIncrementalInfluenciador()\n",
    "\n",
    "    # Se for a primeira vez rodando na vida:\n",
    "    if detector.vetorizador is None:\n",
    "        base_treino = [\n",
    "            \"O pix est√° fora do ar desde cedo\",\n",
    "            \"O pix est√° fora do ar desde cedo\",\n",
    "            \"O pix est√° fora do ar desde cedo\",\n",
    "            \"O pix est√° fora do ar desde cedo\",\n",
    "            \"O pix est√° fora do ar desde cedo\",\n",
    "            \"O pix est√° fora do ar desde cedo\",\n",
    "            \"N√£o consigo realizar transferencia via pix\",\n",
    "            \"Meu cart√£o de cr√©dito ainda n√£o chegou\", \n",
    "            \"Bom dia, gostaria de tirar uma duvida\",\n",
    "            \"O aplicativo √© muito bonito parabens\", \n",
    "            \"O aplicativo √© muito bonito\", \n",
    "            \"Algu√©m me ajuda com o pix por favor\",\n",
    "            \"Algu√©m me ajuda com o pix\",\n",
    "            \"me ajuda com o pix por favor\",\n",
    "        ]\n",
    "        detector.inicializar_modelo(base_treino)\n",
    "\n",
    "    # Simula√ß√£o de dados chegando hoje\n",
    "    dados_hoje = pd.DataFrame({\n",
    "    'id': [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
    "    'texto': [\n",
    "        \"Erro generalizado no pix, arrumem logo\",\n",
    "        \"Pix travado aqui tamb√©m\",\n",
    "        \"Estou aguardando a entrega do cart√£o a dias\",\n",
    "        \"Atendimento p√©ssimo ninguem responde\",\n",
    "        \"Voc√™s s√£o horriveis me atendam\", \n",
    "        \"Vou processar voces pelo atendimento ruim\",\n",
    "        \"O aplicativo √© muito bonito\", \n",
    "        \"Algu√©m me ajuda com o pix por favor\",\n",
    "        \"Algu√©m me ajuda com o pix\",\n",
    "        \"me ajuda com o pix por favor\",\n",
    "        \"O pix est√° fora do ar desde cedo\"\n",
    "\n",
    "    ],\n",
    "    'cpf': ['666', '222', '777', '888', '897', '888', '234', '345', '667', '245', '756']\n",
    "    })\n",
    "\n",
    "    relatorio = detector.processar_lote_diario(dados_hoje)\n",
    "    print(\"\\n--- RESULTADO DO PROCESSAMENTO ---\")\n",
    "    print(relatorio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eee6c09e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_influ' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     protocolos_antigos \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 3. Filtrar apenas o que √© NOVO (O seu \"Anti-Join\")\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m df_novos_casos \u001b[38;5;241m=\u001b[39m \u001b[43mdf_influ\u001b[49m[\u001b[38;5;241m~\u001b[39mdf_influ[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotocolo\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(protocolos_antigos)]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 4. Ajustar nomes de colunas para o Detector (ele espera 'id', 'texto', 'cpf')\u001b[39;00m\n\u001b[0;32m     17\u001b[0m df_novos_casos \u001b[38;5;241m=\u001b[39m df_novos_casos\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotocolo\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexto_cliente\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpf\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpf\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Certifique-se que a coluna CPF existe no seu df_influ\u001b[39;00m\n\u001b[0;32m     21\u001b[0m })\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_influ' is not defined"
     ]
    }
   ],
   "source": [
    "# --- SIMULA√á√ÉO DE CARGA DE DADOS REAIS ---\n",
    "\n",
    "# 1. Supondo que voc√™ carregou seus dados do banco/CSV\n",
    "# df_influ = pd.read_sql(\"SELECT protocolo, texto_cliente, cpf, data_abertura FROM tabela\", conexao)\n",
    "\n",
    "# 2. Carregar o hist√≥rico de protocolos j√° processados (para n√£o repetir trabalho)\n",
    "caminho_processados = \"protocolos_finalizados.csv\"\n",
    "if os.path.exists(caminho_processados):\n",
    "    protocolos_antigos = pd.read_csv(caminho_processados)['protocolo'].tolist()\n",
    "else:\n",
    "    protocolos_antigos = []\n",
    "\n",
    "# 3. Filtrar apenas o que √© NOVO (O seu \"Anti-Join\")\n",
    "df_novos_casos = df_influ[~df_influ['protocolo'].isin(protocolos_antigos)].copy()\n",
    "\n",
    "# 4. Ajustar nomes de colunas para o Detector (ele espera 'id', 'texto', 'cpf')\n",
    "df_novos_casos = df_novos_casos.rename(columns={\n",
    "    'protocolo': 'id',\n",
    "    'texto_cliente': 'texto',\n",
    "    'cpf': 'cpf' # Certifique-se que a coluna CPF existe no seu df_influ\n",
    "})\n",
    "\n",
    "if not df_novos_casos.empty:\n",
    "    detector = DetectorIncrementalInfluenciador()\n",
    "\n",
    "    # Caso seja a PRIMEIRA execu√ß√£o de todas (Cold Start)\n",
    "    if detector.vetorizador is None:\n",
    "        print(\"Iniciando primeira execu√ß√£o hist√≥rica...\")\n",
    "        detector.inicializar_modelo(df_novos_casos['texto'])\n",
    "\n",
    "    # Processamento Incremental\n",
    "    print(f\"Processando {len(df_novos_casos)} novos protocolos...\")\n",
    "    relatorio_final = detector.processar_lote_diario(df_novos_casos)\n",
    "\n",
    "    # 5. Salvar a lista de protocolos processados para o dia de amanh√£\n",
    "    novos_ids = pd.DataFrame({'protocolo': df_novos_casos['id']})\n",
    "    novos_ids.to_csv(caminho_processados, mode='a', index=False, header=not os.path.exists(caminho_processados))\n",
    "    \n",
    "    print(\"Processamento conclu√≠do e salvo.\")\n",
    "else:\n",
    "    print(\"N√£o h√° novos dados para processar hoje.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcf0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> INICIANDO SISTEMA >>>\n",
      "--- Treinando Vectorizer com 9 textos iniciais ---\n",
      "\n",
      ">>> PROCESSANDO DIA 1 (Base Hist√≥rica)...\n",
      "                                        texto  cluster_id  is_influencer  \\\n",
      "0            O pix est√° fora do ar desde cedo           1          False   \n",
      "1  N√£o consigo realizar transferencia via pix           2          False   \n",
      "2      Meu cart√£o de cr√©dito ainda n√£o chegou           3          False   \n",
      "3       Bom dia, gostaria de tirar uma duvida           4          False   \n",
      "4        O aplicativo √© muito bonito parabens           5          False   \n",
      "5                 O aplicativo √© muito bonito           5          False   \n",
      "6         Algu√©m me ajuda com o pix por favor           6          False   \n",
      "7                   Algu√©m me ajuda com o pix           6          False   \n",
      "8                me ajuda com o pix por favor           6          False   \n",
      "\n",
      "         action_log  \n",
      "0       New Cluster  \n",
      "1       New Cluster  \n",
      "2       New Cluster  \n",
      "3       New Cluster  \n",
      "4       New Cluster  \n",
      "5  Existing Cluster  \n",
      "6       New Cluster  \n",
      "7  Existing Cluster  \n",
      "8  Existing Cluster  \n",
      "\n",
      ">>> PROCESSANDO DIA 2 (Novos Casos)...\n",
      "                                         texto  cluster_id  is_influencer  \\\n",
      "0       Erro generalizado no pix, arrumem logo           7          False   \n",
      "1                      Pix travado aqui tamb√©m           7          False   \n",
      "2  Estou aguardando a entrega do cart√£o a dias           8          False   \n",
      "3         Atendimento p√©ssimo ninguem responde           9          False   \n",
      "4               Voc√™s s√£o horriveis me atendam          10          False   \n",
      "5    Vou processar voces pelo atendimento ruim          11          False   \n",
      "6                  O aplicativo √© muito bonito           5           True   \n",
      "7          Algu√©m me ajuda com o pix por favor           6           True   \n",
      "8                    Algu√©m me ajuda com o pix           6           True   \n",
      "9                 me ajuda com o pix por favor           6           True   \n",
      "\n",
      "         action_log  \n",
      "0       New Cluster  \n",
      "1  Existing Cluster  \n",
      "2       New Cluster  \n",
      "3       New Cluster  \n",
      "4       New Cluster  \n",
      "5       New Cluster  \n",
      "6  Existing Cluster  \n",
      "7  Existing Cluster  \n",
      "8  Existing Cluster  \n",
      "9  Existing Cluster  \n",
      "\n",
      ">>> ESTADO FINAL CLUSTER 7 <<<\n",
      "CPFs √∫nicos: {'666', '222'}\n",
      "√â Influencer? False\n"
     ]
    }
   ],
   "source": [
    "# --- SIMULA√á√ÉO COM DADOS MAIS COMPLEXOS ---\n",
    "\n",
    "dados_dia_1 = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'texto': [\n",
    "        \"O pix est√° fora do ar desde cedo\",\n",
    "        \"N√£o consigo realizar transferencia via pix\",\n",
    "        \"Meu cart√£o de cr√©dito ainda n√£o chegou\", \n",
    "        \"Bom dia, gostaria de tirar uma duvida\",\n",
    "        \"O aplicativo √© muito bonito parabens\", \n",
    "        \"O aplicativo √© muito bonito\", \n",
    "        \"Algu√©m me ajuda com o pix por favor\",\n",
    "        \"Algu√©m me ajuda com o pix\",\n",
    "        \"me ajuda com o pix por favor\",\n",
    "    ],\n",
    "    'cpf': ['111', '222', '333', '444', '555', '111', '098', '356', '356']\n",
    "})\n",
    "\n",
    "dados_dia_2 = pd.DataFrame({\n",
    "    'id': [7, 8, 9, 10, 11, 12, 13, 14, 15, 16],\n",
    "    'texto': [\n",
    "        \"Erro generalizado no pix, arrumem logo\",\n",
    "        \"Pix travado aqui tamb√©m\",\n",
    "        \"Estou aguardando a entrega do cart√£o a dias\",\n",
    "        \"Atendimento p√©ssimo ninguem responde\",\n",
    "        \"Voc√™s s√£o horriveis me atendam\", \n",
    "        \"Vou processar voces pelo atendimento ruim\",\n",
    "        \"O aplicativo √© muito bonito\", \n",
    "        \"Algu√©m me ajuda com o pix por favor\",\n",
    "        \"Algu√©m me ajuda com o pix\",\n",
    "        \"me ajuda com o pix por favor\",\n",
    "    ],\n",
    "    'cpf': ['666', '222', '777', '888', '897', '888', '234', '345', '667', '245']\n",
    "})\n",
    "\n",
    "print(\">>> INICIANDO SISTEMA >>>\")\n",
    "detector = IncrementalInfluencerDetector(similarity_threshold=0.6)\n",
    "\n",
    "# Passo 1: Treinamento Inicial (Cold Start)\n",
    "# Usamos o dia 1 para 'aprender' o vocabul√°rio\n",
    "texts_train = [TextPreprocessor.clean(t) for t in dados_dia_1['texto']]\n",
    "detector.fit_vectorizer(texts_train)\n",
    "\n",
    "# Passo 2: Processamento do Dia 1\n",
    "print(\"\\n>>> PROCESSANDO DIA 1 (Base Hist√≥rica)...\")\n",
    "resultado_d1 = detector.process_daily_batch(dados_dia_1)\n",
    "print(resultado_d1[['texto', 'cluster_id', 'is_influencer', 'action_log']])\n",
    "\n",
    "# Passo 3: Processamento do Dia 2 (Incremental)\n",
    "print(\"\\n>>> PROCESSANDO DIA 2 (Novos Casos)...\")\n",
    "# Note que o CPF 555 vai se juntar ao cluster do saque.\n",
    "# Como teremos CPFs 111, 333 e 555 no cluster, ele virar√° Influencer = True\n",
    "resultado_d2 = detector.process_daily_batch(dados_dia_2)\n",
    "print(resultado_d2[['texto', 'cluster_id', 'is_influencer', 'action_log']])\n",
    "\n",
    "# Verificar estado final do Cluster que virou Influencer\n",
    "cluster_saque_id = resultado_d2.iloc[0]['cluster_id']\n",
    "print(f\"\\n>>> ESTADO FINAL CLUSTER {cluster_saque_id} <<<\")\n",
    "print(f\"CPFs √∫nicos: {detector.clusters[cluster_saque_id]['cpfs']}\")\n",
    "print(f\"√â Influencer? {detector.clusters[cluster_saque_id]['is_influencer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cef40d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
