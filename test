#!/usr/bin/env python
# ───────────────────────────────────────────────────────────────────────────────
# keyness_analysis.py
#
# Calcula estatísticas de “keyness” (χ², G‑test e log₂‑ratio) para duas listas
# de contagem de palavras/n‑grams em corpora distintos (ex.: procedente vs.
# improcedente).  100 % livre de modelos pré‑treinados.
#
# Entradas
#   • procedentes.csv      (colunas: word,count)
#   • improcedentes.csv    (colunas: word,count)
#
# Saídas
#   • keyness_results.csv  (tabela completa, ordenada por χ² assinado)
#
# Uso rápido
#   $ python keyness_analysis.py procedentes.csv improcedentes.csv \
#       --min-freq 5 --top 50
# ───────────────────────────────────────────────────────────────────────────────
import argparse
from pathlib import Path

import numpy as np
import pandas as pd
from scipy.stats import chi2
from statsmodels.stats.multitest import multipletests

# ───────────────────────────────────────────────────────────────────────────────
# Funções utilitárias
# ───────────────────────────────────────────────────────────────────────────────
def read_counts(path: Path, label: str) -> pd.DataFrame:
    """Lê CSV no formato word,count e devolve DataFrame normalizado."""
    df = pd.read_csv(path)
    if not {"word", "count"} <= set(df.columns):
        raise ValueError(f"{path.name} deve ter colunas 'word' e 'count'.")
    return df.rename(columns={"count": f"freq_{label}"}).set_index("word")


def merge_corpora(df_a: pd.DataFrame, df_b: pd.DataFrame) -> pd.DataFrame:
    """Outer‑join para obter todas as palavras nos dois corpora."""
    return (
        df_a.join(df_b, how="outer")
        .fillna(0)
        .astype(int)
        .reset_index()
        .rename_axis(None, axis=1)
    )


def compute_keyness(
    df: pd.DataFrame,
    label_a: str = "proc",
    label_b: str = "impr",
    min_freq: int = 5,
) -> pd.DataFrame:
    """Adiciona colunas de keyness (χ² assinado, G‑test, log₂‑ratio, p‑ajustada)."""
    # Totais de tokens em cada corpus
    N_a = df[f"freq_{label_a}"].sum()
    N_b = df[f"freq_{label_b}"].sum()

    # Filtro de frequência mínima
    freq_total = df[f"freq_{label_a}"] + df[f"freq_{label_b}"]
    df = df[freq_total >= min_freq].copy()

    # Contagens observadas
    O11 = df[f"freq_{label_a}"].to_numpy()
    O12 = df[f"freq_{label_b}"].to_numpy()
    O21 = N_a - O11
    O22 = N_b - O12

    # χ² vectorizado (1 g.l.)
    chisq = ((O11 * O22 - O12 * O21) ** 2 * (N_a + N_b)) / (
        (O11 + O12) * (O21 + O22) * (O11 + O21) * (O12 + O22)
    )

    # Log‑likelihood ratio (G‑test)
    eps = 1e-12  # evita log(0)
    g2 = 2 * (
        O11 * np.log(((O11 + eps) * (N_a + N_b)) / ((O11 + O12) * N_a + eps))
        + O12 * np.log(((O12 + eps) * (N_a + N_b)) / ((O11 + O12) * N_b + eps))
        + O21 * np.log(((O21 + eps) * (N_a + N_b)) / ((O21 + O22) * N_a + eps))
        + O22 * np.log(((O22 + eps) * (N_a + N_b)) / ((O21 + O22) * N_b + eps))
    )

    # Log₂‑ratio do efeito
    ratio = (O11 / N_a) / (O12 / N_b + eps)
    log2_ratio = np.log2(ratio + eps)

    # Sinais (positivo → mais comum em A / procedente; negativo → em B / improcedente)
    signed_chisq = np.where(ratio > 1, chisq, -chisq)
    signed_g2 = np.where(ratio > 1, g2, -g2)

    # p‑values e FDR Benjamini–Hochberg
    p_chi = chi2.sf(np.abs(chisq), df=1)
    p_adj = multipletests(p_chi, method="fdr_bh")[1]

    df["chi2_signed"] = signed_chisq
    df["g2_signed"] = signed_g2
    df["log2_ratio"] = log2_ratio
    df["p_adj"] = p_adj

    return df.sort_values("chi2_signed", ascending=False).reset_index(drop=True)


# ───────────────────────────────────────────────────────────────────────────────
# Execução por linha de comando
# ───────────────────────────────────────────────────────────────────────────────
def main() -> None:
    parser = argparse.ArgumentParser(
        description="Calcula estatísticas de keyness entre dois corpora."
    )
    parser.add_argument("proc_csv", type=Path, help="CSV com word,count dos procedentes")
    parser.add_argument(
        "impr_csv", type=Path, help="CSV com word,count dos improcedentes"
    )
    parser.add_argument(
        "--min-freq",
        type=int,
        default=5,
        help="Frequência mínima total para entrar no cálculo (padrão = 5)",
    )
    parser.add_argument(
        "--top",
        type=int,
        default=None,
        help="Número de linhas do topo para salvar no CSV (por padrão salva todas)",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("keyness_results.csv"),
        help="Arquivo de saída (CSV)",
    )
    args = parser.parse_args()

    # 1) Ler contagens
    df_proc = read_counts(args.proc_csv, "proc")
    df_impr = read_counts(args.impr_csv, "impr")

    # 2) Mesclar corpora
    all_words = merge_corpora(df_proc, df_impr)

    # 3) Calcular keyness
    key_df = compute_keyness(all_words, min_freq=args.min_freq)

    # 4) Salvar
    output_df = key_df.head(args.top) if args.top else key_df
    output_df.to_csv(args.output, index=False)
    print(f"✓ Resultados salvos em: {args.output.resolve()}")


if __name__ == "__main__":
    main()
