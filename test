import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# 1. Carregue sua base de dados
df = pd.read_csv("reclamacoes.csv")  # Assuma que há uma coluna 'texto'
textos = df["texto"].astype(str).tolist()

# 2. Transforme os textos em embeddings
model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
embeddings = model.encode(textos, show_progress_bar=True)

# 3. Calcule a matriz de similaridade entre todos os textos
sim_matrix = cosine_similarity(embeddings)

# 4. Para cada texto, conte quantos outros têm similaridade > 0.9
threshold = 0.9
similaridade_alta = (sim_matrix > threshold).astype(int)

# Desconta a similaridade com ele mesmo (valor 1 na diagonal)
np.fill_diagonal(similaridade_alta, 0)
contagem_similares = similaridade_alta.sum(axis=1)

# 5. Normalize para probabilidade (quanto mais similares, maior chance de ser influenciado)
probabilidades = contagem_similares / contagem_similares.max()

# 6. Salve ou insira na base
df["probabilidade_influencer"] = probabilidades

# 7. Visualize os suspeitos
suspeitos = df[df["probabilidade_influencer"] > 0.6]  # Ajuste o cutoff conforme o caso
print(suspeitos[["texto", "probabilidade_influencer"]])

# Opcional: salvar resultado
# df.to_csv("reclamacoes_com_probabilidades.csv", index=False)
