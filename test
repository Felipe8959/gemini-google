# 1) Instalação – execute apenas uma vez
# --------------------------------------------------
# !pip install --upgrade transformers torch sentencepiece -q

# 2) Carregando o modelo e fazendo Fill‑Mask
# --------------------------------------------------
from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline

MODEL_NAME = "neuralmind/bert-base-portuguese-cased"    # apelidado de “BERTimbau‑base”

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model_mlm = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)

preenchimento = pipeline(
    "fill-mask",
    model=model_mlm,
    tokenizer=tokenizer,
    top_k=5,          # quantas sugestões você quer
)

frase = "O Brasil é o maior [MASK] da América do Sul."
print("Frase com máscara:", frase)
print("\nPreenchimentos sugeridos:")
for pred in preenchimento(frase):
    score   = pred["score"]       # probabilidade
    token   = pred["token_str"]   # palavra sugerida
    print(f"{score:.3f} → {token}")

# 3) Gerando embeddings de sentença e medindo similaridade
# --------------------------------------------------
import torch, math
from torch.nn.functional import cosine_similarity

model = model_mlm  # reaproveitamos pesos; só precisamos da parte BERT
model.eval()

def sent_embedding(text: str):
    """Retorna um vetor de 768 dims (pooling médio)."""
    encoded = tokenizer(
        text,
        return_tensors="pt",
        max_length=256,
        truncation=True,
        padding=True,
    )
    with torch.inference_mode():
        out = model.bert(**encoded)[0]          # (batch, seq_len, hidden)
    # mascarar tokens de padding
    attention_mask = encoded["attention_mask"].unsqueeze(-1)
    masked_out = out * attention_mask
    sum_vec = masked_out.sum(dim=1)            # soma ao longo da sequência
    n_tokens = attention_mask.sum(dim=1)        # nº tokens válidos
    return sum_vec / n_tokens                   # média = embedding

txt1 = "O governo anunciou novas medidas econômicas."
txt2 = "Medidas fiscais foram divulgadas pelo presidente."

emb1, emb2 = sent_embedding(txt1), sent_embedding(txt2)
sim = cosine_similarity(emb1, emb2).item()

print(f"\nSimilaridade coseno entre as sentenças: {sim:.3f}")
