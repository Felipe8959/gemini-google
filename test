from pyspark.sql.functions import collect_list, array_distinct

# Coletar todas as palavras em uma lista e manter apenas palavras únicas
vocab_df = tokens_exploded.select("word").distinct()

# Coletar a lista de palavras do vocabulário
vocabulario = vocab_df.select(collect_list("word")).first()[0]

# Criar um dicionário de vocabulário
vocabulario_dict = {word: idx for idx, word in enumerate(vocabulario)}




import numpy as np
from pyspark.sql.types import ArrayType, IntegerType

# UDF para criar vetores de contagem
def count_vectorizer(tokens):
    vector = np.zeros(len(vocabulario), dtype=int)
    for token in tokens:
        if token in vocabulario_dict:
            vector[vocabulario_dict[token]] += 1
    return vector.tolist()

# Registrando a UDF
count_vectorizer_udf = udf(count_vectorizer, ArrayType(IntegerType()))

# Aplicando a UDF para criar a coluna 'vector_cliente'
df_vector_cliente = df_filtered_cliente.withColumn("vector_cliente", count_vectorizer_udf(col("filtered_cliente")))
df_vector_cliente.show(truncate=False)
