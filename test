from pyspark.sql.functions import explode, col

# Explodir os tokens para criar uma lista única de palavras
tokens_exploded = df_filtered_cliente.select(explode(col("filtered_cliente")).alias("word"))

# Criar o vocabulário único (conjunto de palavras)
vocabulario = tokens_exploded.distinct().rdd.flatMap(lambda x: x).collect()
vocabulario_dict = {word: idx for idx, word in enumerate(vocabulario)}



import numpy as np
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, IntegerType

# UDF para criar vetores de contagem
def count_vectorizer(tokens):
    vector = np.zeros(len(vocabulario), dtype=int)
    for token in tokens:
        if token in vocabulario_dict:
            vector[vocabulario_dict[token]] += 1
    return vector.tolist()

# Registrando a UDF
count_vectorizer_udf = udf(count_vectorizer, ArrayType(IntegerType()))

# Aplicando a UDF para criar a coluna 'vector_cliente'
df_vector_cliente = df_filtered_cliente.withColumn("vector_cliente", count_vectorizer_udf(col("filtered_cliente")))
df_vector_cliente.show(truncate=False)
