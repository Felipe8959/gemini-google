# -*- coding: utf-8 -*-
"""
Demo: Complaint text analysis for BACEN with fictitious dataset.

Steps:
1. Generate synthetic dataset of complaints labelled as procedente/improcedente and channel.
2. Pre‑process text (lowercase, remove accents, stopwords, lemmatize (optional)).
3. Exploratory word clouds for each subset.
4. Sentence embeddings (paraphrase-multilingual-MiniLM-L12-v2) and clustering with UMAP + HDBSCAN.
5. Visualize clusters.
6. Build logistic regression classifier on TF‑IDF to predict procedente.
7. Explain model with SHAP.

Requirements (pip install):
  pandas numpy scikit-learn nltk wordcloud sentence-transformers umap-learn hdbscan shap matplotlib
"""

import random
import re
import unicodedata
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import shap
import umap.umap_ as umap
from hdbscan import HDBSCAN
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from wordcloud import WordCloud

# 1. Synthetic dataset -------------------------------------------------------
random.seed(42)
N_SAMPLES = 300

PROCEDENTE_PHRASES = [
    "Cobrança de tarifa indevida no cartão de crédito",
    "Débito não reconhecido em conta corrente",
    "Falha na transferência via PIX",
    "Empréstimo consignado não autorizado",
    "Seguro cobrado sem contratação",
    "Fraude em boleto bancário",
]
IMPROCEDENTE_PHRASES = [
    "Solicitação de aumento de limite negada",
    "Reclamação sobre taxa de juros padrão",
    "Demora no atendimento telefônico",
    "Cartão não aprovado pela análise de crédito",
    "Contestação de score negativado",
    "Reversão de pagamento agendado",
]
VAZAMENTO_PHRASES = [
    "Dados pessoais expostos em link público",
    "Envio de fatura para e‑mail incorreto",
    "SMS contendo CPF enviado a terceiros",
]

NOISE = [
    "",
    "",
    " Cliente insatisfeito.",
    " Situação recorrente.",
    " Preciso de solução urgente.",
]

def random_complaint(label: str) -> str:
    if label == "procedente":
        base = random.choice(PROCEDENTE_PHRASES)
    elif label == "improcedente":
        base = random.choice(IMPROCEDENTE_PHRASES)
    else:
        base = random.choice(VAZAMENTO_PHRASES)
    return f"{base}{random.choice(NOISE)}"

rows = []
for _ in range(N_SAMPLES):
    lbl = random.choices(
        ["procedente", "improcedente", "vazamento"], weights=[0.45, 0.45, 0.10]
    )[0]
    rows.append({"texto": random_complaint(lbl), "status": lbl})

df = pd.DataFrame(rows)
print("Amostra do dataset:")
print(df.head())

# 2. Pre‑processing ----------------------------------------------------------
import nltk
nltk.download("stopwords", quiet=True)
from nltk.corpus import stopwords

STOPWORDS_PT = set(stopwords.words("portuguese"))


def preprocess(text: str) -> str:
    text = text.lower().strip()
    text = unicodedata.normalize("NFKD", text).encode("ascii", "ignore").decode("utf-8")
    tokens = re.findall(r"\b\w+\b", text)
    tokens = [tok for tok in tokens if tok not in STOPWORDS_PT and len(tok) > 2]
    return " ".join(tokens)

df["texto_pp"] = df["texto"].apply(preprocess)

# 3. WordClouds --------------------------------------------------------------

def plot_wordcloud(texts, title):
    wc = WordCloud(width=600, height=400, background_color="white")
    wc.generate(" ".join(texts))
    plt.figure(figsize=(6, 4))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title(title, fontsize=14)
    plt.show()

plot_wordcloud(df["texto_pp"], "WordCloud - Base completa")
for subset_label in df["status"].unique():
    plot_wordcloud(df[df["status"] == subset_label]["texto_pp"], f"WordCloud - {subset_label}")

# 4. Embeddings + Clustering -------------------------------------------------
print("\nGerando embeddings... isto pode levar alguns segundos/minutos.")
model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
embeddings = model.encode(df["texto_pp"], show_progress_bar=True)

reducer = umap.UMAP(n_neighbors=15, min_dist=0.0, metric="cosine", random_state=42)
emb_umap = reducer.fit_transform(embeddings)

clusterer = HDBSCAN(min_cluster_size=5, metric="euclidean")
cluster_labels = clusterer.fit_predict(emb_umap)

df["cluster"] = cluster_labels

plt.figure(figsize=(8, 6))
scatter = plt.scatter(
    emb_umap[:, 0], emb_umap[:, 1], c=cluster_labels, s=30, cmap="tab10", alpha=0.8
)
plt.colorbar(scatter, label="Cluster ID")
plt.title("Clusters de reclamações (UMAP + HDBSCAN)")
plt.xlabel("UMAP‑1")
plt.ylabel("UMAP‑2")
plt.tight_layout()
plt.show()

# 5. Similaridade e deduplicação -------------------------------------------
sim_matrix = cosine_similarity(embeddings)
threshold = 0.85
potential_dupes = np.argwhere(np.triu(sim_matrix, k=1) > threshold)
print(f"Pares de reclamações quase duplicadas (sim > {threshold}): {len(potential_dupes)}")

# 6. Classificador procedente vs improcedente -------------------------------
binary_df = df[df["status"].isin(["procedente", "improcedente"])]
y = binary_df["status"].map({"procedente": 1, "improcedente": 0}).values
X_text = binary_df["texto_pp"]

vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))
X = vectorizer.fit_transform(X_text)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, stratify=y, random_state=42
)

clf = LogisticRegression(max_iter=1000)
clf.fit(X_train, y_train)
print("\nRelatório de classificação (procedente=1, improcedente=0):")
print(classification_report(y_test, clf.predict(X_test)))

# 7. Explicabilidade com SHAP ----------------------------------------------
print("\nGerando explicações SHAP...")
explainer = shap.LinearExplainer(clf, X_train, feature_dependence="independent")
shap_values = explainer.shap_values(X_test)
shap.summary_plot(
    shap_values,
    features=X_test,
    feature_names=vectorizer.get_feature_names_out(),
    show=False,
)
plt.title("SHAP resumo - tokens mais influentes")
plt.show()

# 8. Exportar resultados ----------------------------------------------------
output_dir = Path("outputs")
output_dir.mkdir(exist_ok=True)

df.to_csv(output_dir / "bacen_sintetico.csv", index=False)
print(f"\nBase sintética salva em: {output_dir / 'bacen_sintetico.csv'}")
